{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n[nltk_data]     D:\\Users\\PDudarin\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     D:\\Users\\PDudarin\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\ng_path >>  D:/Projects/Aspirantura/GASU_SP/\ng_max_inds_count >>  19995\ng_th >>  0.4\nDone\n"
     ]
    }
   ],
   "source": [
    "#IDEA построить гистограмму распределения слов у Counter\n",
    "#\n",
    "\n",
    "\n",
    "#                Дальше идет работы с графом с учетом полученного массива сематической близости пар слов\n",
    "#                          Соствление нечеткого графа по моногородам  (по словам)\n",
    "\n",
    "#  Для построения графа если семантика уже собрана то можно начинать с этого места\n",
    "\n",
    "# preparation\n",
    "import pymorphy2\n",
    "import nltk\n",
    "import pandas as pd\n",
    "nltk.download('punkt')  #не знаю нужно ли каждый раз это скачивать\n",
    "nltk.download('stopwords')  #не знаю нужно ли каждый раз это скачивать\n",
    "\n",
    "\n",
    "# Set global params and praparation\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "g_path = \"D:/Projects/Aspirantura/GASU_SP/\"\n",
    "print(\"g_path >> \",g_path)\n",
    "g_max_inds_count = 19995 #19995\n",
    "print(\"g_max_inds_count >> \",g_max_inds_count)\n",
    "g_th = 0.4  #start threshold for fuzzy graph\n",
    "print(\"g_th >> \",g_th)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Start time Thu, 30 Mar 2017 07:24:40 ---\nStart creating Indictors DataSet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кол-во стоп слов:  396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indicators >> [[0, 'ДОЛЯ НАСЕЛЕНИЯ ИМЕЮЩЕГО ДЕНЕЖНЫЕ ДОХОДЫ НИЖЕ ВЕЛИЧИНЫ ПРОЖИТОЧНОГО МИНИМУМА В ОБЩЕЙ ЧИСЛЕННОСТИ НАСЕЛЕНИЯ АЛЕЙСКОГО РАЙОНА АЛТАЙСКОГО КРАЯ', ['население', 'денежный', 'доход', 'ниже', 'величина', 'прожиточный', 'население', 'алейский', 'район', 'край'], {}], [1, 'КОЛИЧЕСТВО ИСТОРИКОПАТРИОТИЧЕСКИХ ГЕРОИКОПАТРИОТИЧЕСКИХ И ВОЕННОПАТРИОТИЧЕСКИХ МУЗЕЕВ ИЛИ МУЗЕЙНЫХ КОМНАТ В ОБЩЕОБРАЗОВАТЕЛЬНЫХ УЧРЕЖДЕНИЯХ', ['историкопатриотический', 'героикопатриотический', 'военнопатриотический', 'музей', 'музейный', 'комната', 'общеобразовательный'], {}], [2, 'ОХВАТ ПРОФИЛАКТИЧЕСКИМИ МЕРАМИ ПОДРОСТКОВ И МОЛОДЕЖИ В ВОЗРАСТЕ ОТ ДО ЛЕТ', ['охват', 'профилактический', 'подросток', 'молодёжь', 'возраст'], {}], [3, 'УРОВЕНЬ ЗАНЯТОСТИ НАСЕЛЕНИЯ', ['занятость', 'население'], {}], [4, 'УДЕЛЬНЫЙ ВЕС РАБОТНИКОВ С ПРОФЕССИОНАЛЬНЫМ ОБРАЗОВАНИЕМ В ОБЩЕЙ ЧИСЛЕННОСТИ ЗАНЯТЫХ В ЭКОНОМИКЕ', ['вес', 'работник', 'профессиональный', 'занятой', 'экономика'], {}], [5, 'ДОЛЯ ВЫПУСКНИКОВ ОБЩЕОБРАЗОВАТЕЛЬНЫХ ОРГАНИЗАЦИЙ ПОСТУПИВШИХ В ПРОФЕССИОНАЛЬНЫЕ ОБРАЗОВАТЕЛЬНЫЕ ОРГАНИЗАЦИИ', ['выпускник', 'общеобразовательный', 'поступить', 'профессиональный', 'образовательный'], {}], [6, 'ОБЪЕМ ОТГРУЖЕННЫХ ТОВАРОВ СОБСТВЕННОГО ПРОИЗВОДСТВА ВЫПОЛНЕННЫХ РАБОТ УСЛУГ СОБСТВЕННЫМИ СИЛАМИ ПО КРУГУ КРУПНЫХ И СРЕДНИХ ОРГАНИЗАЦИЙ', ['объесть', 'отгрузить', 'товар', 'собственный', 'производство', 'выполнить', 'работа', 'услуга', 'собственный', 'сила', 'круг', 'крупный'], {}], [7, 'ОБЪЕМ ИНВЕСТИЦИЙ ЗА СЧЕТ ВСЕХ ИСТОЧНИКОВ ФИНАНСИРОВАНИЯ БЕЗ СУБЪЕКТОВ МАЛОГО ПРЕДПРИНИМАТЕЛЬСТВА И ОБЪЕМОВ ИНВЕСТИЦИЙ НЕ НАБЛЮДАЕМЫХ ПРЯМЫМИ СТАТИСТИЧЕСКИМИ МЕТОДАМИ', ['объесть', 'инвестиция', 'счёт', 'источник', 'финансирование', 'субъект', 'предпринимательство', 'инвестиция', 'наблюдать', 'прямая', 'статистический', 'метод'], {}], [8, 'УВЕЛИЧЕНИЕ ЧИСЛА ДЕТЕЙ ОЗДОРОВЛЕННЫХ В ПОЕЗДКАХ С РОДИТЕЛЯМИ', ['ребёнок', 'оздоровить', 'поездка', 'родитель'], {}], [9, 'ДОЛЯ УЧАЩИХСЯ ШКОЛ ГОРОДА УЧАСТВУЮЩИХ В ГОРОДСКИХ И КРАЕВЫХ МЕРОПРИЯТИЯХ ПАТРИОТИЧЕСКОЙ НАПРАВЛЕННОСТИ', ['учащийся', 'школа', 'город', 'участвовать', 'городской', 'краевой', 'мероприятие', 'патриотический', 'направленность'], {}]]\nIndicators saved in  D:/Projects/Aspirantura/GASU_SP/monocity_inds_19995.pkl\n\n--------------------------------------------------------\n\nStart creating Words DataSet\nl_cntr_mean >>  22.241721349\nl_cntr_std >>  84.0264187792\nWords cnt:  6553\nWords (normalized) [0:10]>>  [['сооружение', 2.1036036191838656], ['информатизация', -0.2051940520552924], ['заполняемость', -0.25279812775094512], ['районный', 0.8420956132490679], ['ввод', 2.8414667924664831], ['налог', 1.5561567486838592], ['профориентация', -0.25279812775094512], ['ремонтный', -0.12188691958790011], ['гбуз', -0.25279812775094512], ['ограничение', -0.10998590066398693]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words with frequencies saved in  D:/Projects/Aspirantura/GASU_SP/monocity_words_19995.csv\n\n--------------------------------------------------------\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available processed pair count:  21869191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words DS saved in  D:/Projects/Aspirantura/GASU_SP/monocity_words_ds_19995.pkl\nWords dictionary saved in  D:/Projects/Aspirantura/GASU_SP/monocity_words_19995.pkl\nWORDS_DS[5][0:11] example:  [0.0186236916537, 0.0675680233711, 0.119470180412, 0.0709973016005, 0.037454129799, 1, 0.0801358016419, 0.00246889946354, 0.0256127526734, 0.416408220953, 0.248680515419, 0.147238670951, 0.113274846876, 0.00842275648886, 0.188756284591, 0.109573469217, 0, 0, 0.0428798598092, 0.103593132204]\nDone\n--- 9241.772263765335 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Составление справочника слов и формирование дата сета (матрицы графа) слов и списков показателей и слов\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pymorphy2\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import random\n",
    "import collections\n",
    "\n",
    "print(\"--- Start time %s ---\" % strftime(\"%a, %d %b %Y %H:%M:%S\", gmtime()))\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Start creating Indictors DataSet\")\n",
    "\n",
    "#Загрузка даных\n",
    "xl_file = pd.ExcelFile(g_path+'Показатели моногородов.xlsx')\n",
    "ds = xl_file.parse(\"Лист1\")\n",
    "\n",
    "\n",
    "#clear symbols\n",
    "# эти символы разбивают слово на два\n",
    "chars_to_remove = [u'«', u'»', u'!', u'<', u'>', u'?', u',', u'.', u'-', u'(', u')', u'[', u']', u'\"']\n",
    "dd = {ord(c):' ' for c in chars_to_remove}\n",
    "# эти символы убирают слово из сравнения\n",
    "stop_sybols = [u'/', u'\\\\', u'№', u':', u'1', u'2', u'3', u'4' , u'5', u'6', u'7', u'8', u'9', u'0', u'–']\n",
    "\n",
    "# Загружаем стоп слова\n",
    "xl_file = pd.ExcelFile(g_path+'Стоп слова.xlsx')\n",
    "# dfs = {sheet_name: xl_file.parse(sheet_name)\n",
    "#           for sheet_name in xl_file.sheet_names}\n",
    "ds_stop_words = xl_file.parse(\"Лист1\")\n",
    "stop_list = [morph.parse(x)[0].normal_form for x in ds_stop_words.STOP_WORDS.str.upper().tolist() ]\n",
    "# add common stop words\n",
    "for w in stopwords.words('russian'):\n",
    "    stop_list.append(w)\n",
    "print(\"Кол-во стоп слов: \",len(stop_list))\n",
    "\n",
    "\n",
    "# List of indicators\n",
    "inds = []\n",
    "i = 0\n",
    "\n",
    "l_inds_list = ds.IND_NAME.str.upper().tolist()\n",
    "if g_max_inds_count < len(l_inds_list)*0.9:\n",
    "  sample_list = random.sample(l_inds_list, g_max_inds_count)\n",
    "else:\n",
    "  sample_list = l_inds_list[0:g_max_inds_count]\n",
    "\n",
    "for j in sample_list:\n",
    "    inds.append([i, j, [], {}])\n",
    "    inds[i][2] = [morph.parse(x)[0].normal_form for x in nltk.word_tokenize(j.translate(dd))\n",
    "                  if (not any(st in x for st in stop_sybols)) and (morph.parse(x)[0].normal_form not in stop_list)]\n",
    "    i+=1\n",
    "\n",
    "print(\"indicators >>\", inds[0:10])\n",
    "\n",
    "output = open(g_path+'monocity_inds_'+str(g_max_inds_count)+'.pkl', 'wb')\n",
    "pickle.dump(inds, output)\n",
    "output.close()\n",
    "print(\"Indicators saved in \", g_path+'monocity_inds_'+str(g_max_inds_count)+'.pkl')\n",
    "print(\"\\n--------------------------------------------------------\\n\")\n",
    "\n",
    "print(\"Start creating Words DataSet\")\n",
    "l_words_counter = collections.Counter([w for ind in inds for w in ind[2]]) # получили слова с их количествами\n",
    "# print(\"l_words_counter \", l_words_counter)\n",
    "\n",
    "# теперь нормируем важность слова относительно\n",
    "l_values = [int(v)  for k,v in dict(l_words_counter).items()]\n",
    "l_cntr_mean = np.mean(l_values)\n",
    "l_cntr_std = np.std(l_values)\n",
    "print(\"l_cntr_mean >> \", l_cntr_mean)\n",
    "print(\"l_cntr_std >> \", l_cntr_std)\n",
    "\n",
    "# l_words_dict = {k: (v-l_cntr_mean)/l_cntr_std  for k,v in dict(l_words_counter).items()}\n",
    "l_words_dict = [ [k, (v-l_cntr_mean)/l_cntr_std]  for k,v in dict(l_words_counter).items()]\n",
    "# words = list(set())  #delete duplicates\n",
    "\n",
    "print(\"Words cnt: \", len(l_words_dict))\n",
    "\n",
    "# l_dt_smpl = {}\n",
    "# i = 0\n",
    "# for k,v in l_words_dict.items():\n",
    "#     i +=1\n",
    "#     if i < 10:\n",
    "#         l_dt_smpl[k] = v\n",
    "#     else:\n",
    "#         break\n",
    "print(\"Words (normalized) [0:10]>> \", l_words_dict[0:10])\n",
    "\n",
    "#вывести все слова с частотами в отдельный текстовый файл\n",
    "l_df = pd.DataFrame(l_words_dict)\n",
    "l_df.to_csv(g_path+'monocity_words_'+str(g_max_inds_count)+'.csv', index=False, header=True)\n",
    "print(\"Words with frequencies saved in \", g_path+'monocity_words_'+str(g_max_inds_count)+'.csv')\n",
    "\n",
    "print(\"\\n--------------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "# words = list(l_words_dict.keys())\n",
    "l_words_cnt = len(l_words_dict)\n",
    "\n",
    "words_ds = [[0 for j in range(0, l_words_cnt)] for i in range(0, l_words_cnt)]\n",
    "\n",
    "# IDEA может быть повышать важность (силу) связей между словами на основании важности сами слов (частота или еще как) ?\n",
    "\n",
    "# Semantic dictionary\n",
    "pkl_file = open(g_path+'words_dict_monocities_sem.pkl', 'rb')\n",
    "l_words_sem_dist_dict = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "l_sem_dict_empty = {k:v for k, v in l_words_sem_dist_dict.items() if v is None } #all not empty\n",
    "print(\"Available processed pair count: \", len(l_words_sem_dist_dict)-len(l_sem_dict_empty))\n",
    "\n",
    "\n",
    "for i in range(0, l_words_cnt):\n",
    "    for j in range(0, l_words_cnt):\n",
    "        if l_words_dict[i][0] == l_words_dict[j][0]:  # FIXME actually only when i==j\n",
    "            words_ds[i][j] = 1\n",
    "            words_ds[j][i] = 1\n",
    "        else:\n",
    "            mc=\"\"\n",
    "            m = 0\n",
    "            w1 = l_words_dict[i][0]\n",
    "            w2 = l_words_dict[j][0]\n",
    "            try:\n",
    "                if  w1 > w2:\n",
    "                    mc=l_words_sem_dist_dict[w1+\"__\"+w2]\n",
    "                else:\n",
    "                    mc=l_words_sem_dist_dict[w2+\"__\"+w1]\n",
    "                if mc not in [None, \"Error\", \"Unknown\"]:\n",
    "                    m = abs(float(mc))  # [-1;1] !!!\n",
    "            except:\n",
    "                None\n",
    "\n",
    "            words_ds[i][j] = m\n",
    "            words_ds[j][i] = m\n",
    "\n",
    "\n",
    "# # добавим сильные связи слов внутри одного показателя\n",
    "# j = 0\n",
    "# m = 1\n",
    "# for ind in inds:\n",
    "#     for j1 in range(0, len(ind[2])):\n",
    "#         if j1>0:\n",
    "#             words_ds[j][j-1] = m\n",
    "#             words_ds[j-1][j] = m\n",
    "#\n",
    "#         j+=1\n",
    "\n",
    "output = open(g_path+'monocity_words_ds_'+str(g_max_inds_count)+'.pkl', 'wb')\n",
    "pickle.dump(words_ds, output)\n",
    "output.close()\n",
    "print(\"Words DS saved in \", g_path+'monocity_words_ds_'+str(g_max_inds_count)+'.pkl')\n",
    "\n",
    "\n",
    "output = open(g_path+'monocity_words_'+str(g_max_inds_count)+'.pkl', 'wb')\n",
    "pickle.dump(l_words_dict, output)\n",
    "output.close()\n",
    "print(\"Words dictionary saved in \", g_path+'monocity_words_'+str(g_max_inds_count)+'.pkl')\n",
    "\n",
    "print(\"WORDS_DS[5][0:11] example: \", words_ds[5][0:20])\n",
    "\n",
    "print(\"Done\")\n",
    "end_time = time.time()\n",
    "print(\"--- %s seconds ---\" % (end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Start time Thu, 30 Mar 2017 11:30:22 ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words_ds len:  6553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words len:  6553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excluded cnt:  2100\nExclude list [0:50]:  ['устранить', 'исток', 'архитектурноградостроительный', 'увеличесние', 'лишить', 'выполненый', 'увелмчение', 'гшс', 'предоставить', 'продолжить', 'семьигражданин', 'упарвление', 'предпрития', 'пвр', 'показать', 'тысчеловек', 'учебновоспитательный', 'лёгкое', 'разделить', 'путёвка', 'спортивномассовый', 'предпринимательтсв', 'отнести', 'среднеспециальный', 'государственнуюмуниципальный', 'directum', 'финансовокредитный', 'опечение', 'учрежденийя', 'населенияпроживать', 'барс', 'проведнный', 'безъбарьерный', 'каквинский', 'программноцелевой', 'переселеныгородский', 'застроить', 'отметить', 'перерасчёт', 'информационноконсультационный', 'аварийнотехнический', 'неструдостпособность', 'распорядителейсредство', 'среденемесячный', 'ндфл', 'первое', 'фот', 'оказаться', 'тоспа', 'освещённость', 'причитать', 'возвести', 'удовлетворнный', 'часто', 'выше', 'звукофикация', 'коррупциогенный', 'нормативноправовой', 'постановить', 'выплнения', 'дежурнодиспетчерский', 'социальнотрудов', 'гяровой', 'гоодскома', 'геоинформационный', 'далеести', 'чебаркульский', 'g', 'совместителеймалый', 'терристический', 'лесом', 'затратить', 'висимоуткинск', 'отчётность', 'утвержденый', 'твинформация', 'возникнуть', 'производствавыполненный', 'холоджный', 'галексин', 'недоремонт', 'отобщий', 'оснащнность', 'миниципальный', 'набрать', 'осветить', 'наёмный', 'хозяйственноэксплуатационный', 'бизнесинкубатор', 'доставить', 'db', 'среднесписачный', 'телевид', 'социальнопсихологический', 'превысить', 'волонтеровпропагандист', 'задолжность', 'отчтность', 'vipnetкоординатор', 'учта']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph D:/Projects/Aspirantura/GASU_SP/monocity_words_graph_19995_40.graphml created.\nIGRAPH UNW- 4453 36194 -- \n+ attr: name (v), norm_weight (v), weight (e)\nDone\n--- 29.793979167938232 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Construct fuzzy graph from WORDS\n",
    "import igraph\n",
    "# from igraph import *\n",
    "import pickle\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "\n",
    "print(\"--- Start time %s ---\" % strftime(\"%a, %d %b %Y %H:%M:%S\", gmtime()))\n",
    "start_time = time.time()\n",
    "\n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "# def calc_weigth(p_vw1, p_vw2, p_ew) -> float:\n",
    "#     l_vw = max(p_vw1, p_vw2)\n",
    "#     if l_vw > 0:\n",
    "#         l_w = min(1, p_ew + l_vw/10)\n",
    "#         return l_w\n",
    "#     else:\n",
    "#         return p_ew\n",
    "\n",
    "def calc_weigth(p_vw1, p_vw2, p_ew) -> float:\n",
    "    return p_ew\n",
    "\n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "def create_graph(p_words_ds, p_words, p_edge_treshold, p_graph_file_name):\n",
    "    graph_ver_cnt = len(p_words)\n",
    "    g = igraph.Graph()\n",
    "    g.add_vertices(graph_ver_cnt)\n",
    "    # g.vs[\"name\"] = [p_words[i] for i in range(0, graph_ver_cnt)]\n",
    "    g.vs[\"name\"] = [k[0] for k in p_words]\n",
    "    g.vs[\"norm_weight\"] = [k[1] for k in p_words]\n",
    "    # g.vs[\"cluster_n\"] = [0 for i in range(0, graph_ver_cnt)]\n",
    "    # g.vs[\"cluster_n\"] = [0 for k in p_words.items()]\n",
    "\n",
    "    edgs = [ (i,j) for i in range(0, graph_ver_cnt) for j in range(0, graph_ver_cnt)\n",
    "             if i>j and p_words_ds[i][j] >= p_edge_treshold]\n",
    "    # print(edgs)\n",
    "    g.add_edges(edgs)\n",
    "\n",
    "    g.es[\"weight\"] = [ calc_weigth(p_words[i][1], p_words[j][1], p_words_ds[i][j])\n",
    "                         for i in range(0, graph_ver_cnt)\n",
    "                           for j in range(0, graph_ver_cnt) if i>j and p_words_ds[i][j] > p_edge_treshold\n",
    "                     ]\n",
    "\n",
    "\n",
    "    # delete isolated vertices\n",
    "    l_exclude_list = []\n",
    "    l_exclude_vs = []\n",
    "    for i in reversed(range(0,graph_ver_cnt)):\n",
    "        if g.vs[i].degree() == 0:\n",
    "           l_exclude_list.append(g.vs[i][\"name\"])\n",
    "           l_exclude_vs.append(i)\n",
    "\n",
    "    g.delete_vertices(l_exclude_vs)\n",
    "\n",
    "    print(\"Excluded cnt: \", len(l_exclude_list))\n",
    "    print(\"Exclude list [0:50]: \", l_exclude_list[0:100])\n",
    "    g.write_graphml(p_graph_file_name)\n",
    "    print(\"Graph \"+p_graph_file_name+\" created.\")\n",
    "    igraph.summary(g)\n",
    "    return g\n",
    "\n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "pkl_file = open(g_path+'monocity_words_ds_'+str(g_max_inds_count)+'.pkl', 'rb')\n",
    "words_ds = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "print(\"words_ds len: \", len(words_ds))\n",
    "\n",
    "pkl_file = open(g_path+'monocity_words_'+str(g_max_inds_count)+'.pkl', 'rb')\n",
    "words = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "print(\"words len: \", len(words))\n",
    "\n",
    "# for th in [0.5, 0.7, 0.9]:\n",
    "# for th in [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "for th in [g_th]:\n",
    "    l_graph_main = create_graph(words_ds, words, th,\n",
    "                                g_path+\"monocity_words_graph_\"\n",
    "                                +str(g_max_inds_count)+\"_\"+str(round(th*100))+\".graphml\"\n",
    "                               )\n",
    "\n",
    "print(\"Done\")\n",
    "end_time = time.time()\n",
    "print(\"--- %s seconds ---\" % (end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words_ds len:  6573\nl_cnt >>  0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'ребенок__девочка'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-f87169930329>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[1;31m#       l_cnt +=1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"l_cnt >> \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml_cnt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml_words_sem_dist_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ребенок__девочка\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[1;31m# print(l_words_sem_dist_dict[\"девочка__ребёнок\"])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml_words_sem_dist_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"факт__оборот\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ребенок__девочка'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Проверки. Временное\n",
    "\n",
    "\n",
    "pkl_file = open(g_path+'monocity_words_ds_'+str(g_max_inds_count)+'.pkl', 'rb')\n",
    "words_ds = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "print(\"words_ds len: \", len(words_ds))\n",
    "\n",
    "# pkl_file = open(g_path+'monocity_words_'+str(g_max_inds_count)+'.pkl', 'rb')\n",
    "# words = pickle.load(pkl_file)\n",
    "# pkl_file.close()\n",
    "# print(\"words len: \", len(words))\n",
    "\n",
    "l_cnt = 0 \n",
    "# for ws in words_ds:\n",
    "#   if max(w for w in ws if w != 1)  < 0.001:\n",
    "#       l_cnt +=1\n",
    "print(\"l_cnt >> \", l_cnt)\n",
    "print(l_words_sem_dist_dict[\"ребёнок__девочка\"])\n",
    "# print(l_words_sem_dist_dict[\"девочка__ребёнок\"])\n",
    "print(l_words_sem_dist_dict[\"факт__оборот\"])\n",
    "# print(l_words_sem_dist_dict[\"оборот__факт\"])\n",
    "# print(\"None cnt >> \", sum(1 for k,v in l_words_sem_dist_dict.items() if v==None))\n",
    "# print(\"Error cnt >> \",sum(1 for k,v in l_words_sem_dist_dict.items() if v=='Error'))\n",
    "# print(\"Unknown cnt >> \", sum(1 for k,v in l_words_sem_dist_dict.items() if v=='Unknown'))\n",
    "\n",
    "i = 0\n",
    "print(\"Unknown pairs:\")\n",
    "for k,v in l_words_sem_dist_dict.items():\n",
    "    if v == \"Unknown\":\n",
    "      print(k)\n",
    "      i += 1\n",
    "    if i > 10:\n",
    "        break\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Start time Thu, 06 Apr 2017 14:43:57 ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main graph summary:  IGRAPH UNW- 4453 36194 -- \n+ attr: id (v), name (v), norm_weight (v), weight (e)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hier graph summary:  IGRAPH UN-- 149 148 -- \n+ attr: child_cnt (v), child_node_cnt (v), graph (v), keywords (v), layer (v), layer_n (v), name (v), parent_node (v)\nGraphs' samples:  ['n', 'n0', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n10', 'n12', 'n13']\nGraphs' in hierarchy:  149\nGraphs' samples:  [<igraph.Graph object at 0x0000000014F1E408>, <igraph.Graph object at 0x000000000D8836D8>, <igraph.Graph object at 0x0000000014F345E8>, <igraph.Graph object at 0x0000000014F1EC78>, <igraph.Graph object at 0x000000000C1638B8>]\nOne graph words sample:  ['сооружение', 'информатизация', 'заполняемость', 'районный', 'ввод', 'налог', 'профориентация', 'ремонтный', 'гбуз', 'ограничение']\nGraph file: D:/Projects/Aspirantura/GASU_SP/monocity_words_hier_graph_19995_40.graphml\nDone\n--- 53.571356534957886 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Hierarchical clustering based on fuzzy connectedness\n",
    "\n",
    "import igraph\n",
    "# from igraph import *\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "from time import gmtime, strftime\n",
    "from typing import List\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "\n",
    "def del_edges(p_graph: igraph.Graph, p_edge_threshold: float):\n",
    "\n",
    "    for e in p_graph.es:\n",
    "        if e[\"weight\"] < p_edge_threshold:\n",
    "            p_graph.delete_edges(e.index)\n",
    "\n",
    "    # for v in p_graph.vs:\n",
    "    #     if v.degree() == 0:\n",
    "    #         p_graph.delete_vertices(v.index)\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "\n",
    "def get_subgraph_weight(p_graph: igraph.Graph) -> float:\n",
    "\n",
    "    # l_degree = p_graph.degree()\n",
    "    l_norm_weight = [1 if v <= 0 else v+1 for v in p_graph.vs[\"norm_weight\"]]\n",
    "    # l_name = p_graph.vs[\"name\"]\n",
    "    l_weight = sum(l_norm_weight[i] for i in range(0, len(p_graph.vs)) )\n",
    "\n",
    "    return l_weight\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "\n",
    "def get_subgraph_keywords(p_graph: igraph.Graph, p_keyword_cnt: int) -> List[str]:\n",
    "\n",
    "    l_degree = p_graph.degree()\n",
    "    l_norm_weight = [1 if v <= 0 else v+1 for v in p_graph.vs[\"norm_weight\"]]\n",
    "    l_name = p_graph.vs[\"name\"]\n",
    "\n",
    "    l_dict = {l_name[i]: l_degree[i]*l_norm_weight[i] for i in range(0, len(p_graph.vs)) }\n",
    "\n",
    "    return sorted(l_dict, key=l_dict.get, reverse=True)[0:min(p_keyword_cnt, len(p_graph.vs))]\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "\n",
    "#возвращает разрезанный граф, всегда больше одного подграфа,\n",
    "# за исключением случая, когда разрезание противоречит органичениям\n",
    "# старается получить разбиение с заданным количеством значимых (те которые превратятся в узлы) подграфов\n",
    "def cut_graph(pGraph: igraph.Graph, p_edge_threshold: float,\n",
    "              p_edge_th_step: float,\n",
    "              p_max_edge_threshold: float,\n",
    "              p_avg_bouquet: int,\n",
    "              p_min_subgraph_coeff: float = 10 #коэффициент при котором субграф добавляется в иерархию\n",
    "             ) -> [[igraph.Graph], float]:\n",
    "\n",
    "    l_edge_threshold = p_edge_threshold\n",
    "    l_prev_sgs_cnt = 1\n",
    "    l_prev_sgs =  [pGraph]\n",
    "    l_sgs = [pGraph]\n",
    "\n",
    "    #пока возможно разбиение\n",
    "    while (l_edge_threshold<1) and (l_edge_threshold < p_max_edge_threshold):\n",
    "      del_edges(pGraph, l_edge_threshold)\n",
    "      l_comps = pGraph.components(mode='STRONG')  #Returns:a VertexClustering object\n",
    "      l_sgs = l_comps.subgraphs()\n",
    "      l_sgs_cnt = sum(1 if (get_subgraph_weight(sg) >= p_min_subgraph_coeff) else 0 for sg in l_sgs)\n",
    "\n",
    "      #подходит ли нам такое рзбиение?\n",
    "      #если разбиаение подошло то выходим из цикла\n",
    "      if (l_prev_sgs_cnt == 1) and (l_sgs_cnt >= p_avg_bouquet):\n",
    "          break\n",
    "      else:\n",
    "          # единственная ситуация продолжения разбиения: не достигли среднего\n",
    "          if (l_prev_sgs_cnt == 1) and (l_sgs_cnt < p_avg_bouquet):\n",
    "              l_prev_sgs_cnt = l_sgs_cnt\n",
    "              l_prev_sgs = l_sgs\n",
    "          else:\n",
    "            # если отклонение от среднего количества на предыдущем шаге было меньше, то возвращаем его\n",
    "            if abs(l_prev_sgs_cnt - p_avg_bouquet) < abs(l_sgs_cnt - p_avg_bouquet):\n",
    "              l_sgs = l_prev_sgs\n",
    "              break\n",
    "            # достигои идеального количества подграфов\n",
    "            else:\n",
    "              break\n",
    "\n",
    "      #шаг для следующего разбиения\n",
    "      l_edge_threshold += p_edge_th_step\n",
    "\n",
    "    # if abs(l_edge_threshold - p_edge_threshold) > 10E-4:\n",
    "    #     print(\"Different th \", l_edge_threshold, \" \", p_edge_threshold)\n",
    "    return [l_sgs, l_edge_threshold]\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "\n",
    "def add_layer(p_hier_graph: igraph.Graph, p_graph: igraph.Graph, p_parent_vtx: str,\n",
    "              p_layer_n: int,\n",
    "              p_edge_threshold: float, #уровень с которого нужно начинать разбивать граф (для первого случая это текущий уровень)\n",
    "              p_edge_th_step: float,\n",
    "              p_max_edge_threshold: float,\n",
    "              p_max_layer_cnt: int,\n",
    "              p_min_subgraph_coeff: float = 10, #коэффициент при котором субграф добавляется в иерархию\n",
    "              p_keywords_cnt: int = 10, #количество ключевых слов для узла\n",
    "              p_keyword_coeff: float = 100, #коэффициент значимости первого слова от каждого подграфа\n",
    "              p_avg_bouquet: int = 4\n",
    "             ) -> {str: int}:\n",
    "\n",
    "\n",
    "    #нарежем граф так, чтобы было нужно число подграфов (получим один подграф, только если разрезать не возможно)\n",
    "    l_edge_threshold = p_edge_threshold\n",
    "    if p_layer_n > 1: #первый шаг уже приходит нарезанным, поэтому для него условие ELSE\n",
    "        [sgs, l_edge_threshold] = cut_graph(p_graph, l_edge_threshold,  p_edge_th_step, p_max_edge_threshold, p_avg_bouquet, p_min_subgraph_coeff)\n",
    "    else: #первый шаг особенный\n",
    "        l_comps = p_graph.components(mode='STRONG')  #Returns:a VertexClustering object\n",
    "        sgs = l_comps.subgraphs()\n",
    "        if len(sgs) == 1: #если в самом начале нам дали не разбитый граф, то нужно его тоже разбить\n",
    "            [sgs, l_edge_threshold] = cut_graph(p_graph, l_edge_threshold+p_edge_th_step,  p_edge_th_step, p_max_edge_threshold, p_avg_bouquet, p_min_subgraph_coeff)\n",
    "\n",
    "    #устанавливаем начальные переменные\n",
    "    l_keywords = {}\n",
    "    l_prnt_index = len(p_hier_graph.vs)-1\n",
    "    l_node_cnt = 1 # если считать с нуля то будет умножение на ноль и пропадут многие ключевые сслова\n",
    "\n",
    "    #проходимся по всем подграфам\n",
    "    for sg in sgs:\n",
    "        l_sg_keywords = {}\n",
    "        # l_sg_vrtx_indx = 0\n",
    "        # если вес данного подграфа достин собственного узла в иерархии то\n",
    "        # также у нас теперь не может граф распадаться только на одну вершину, мы этого не допускаем процедурой разрезания\n",
    "        # if (get_subgraph_weight(sg) >= p_min_subgraph_coeff) or (len(sgs) == 1):\n",
    "        if (get_subgraph_weight(sg) >= p_min_subgraph_coeff) and (len(sgs) != 1):\n",
    "            # Add vertex\n",
    "\n",
    "            # TODO от такого именования нужно будет уйти на более абстрактное, когда будут ключевые слова сделаны\n",
    "            if len(sg.vs) > p_keywords_cnt:\n",
    "              l_vrtx_name = \"Layer \"+str(p_layer_n)+\" \"+' '.join(list(random.sample(sg.vs[\"name\"], 3)))\n",
    "            else:\n",
    "              l_vrtx_name = \"Layer \"+str(p_layer_n)+\" \"+' '.join(list(sg.vs[\"name\"]))\n",
    "\n",
    "            p_hier_graph.add_vertex(l_vrtx_name)\n",
    "            l_sg_vrtx_indx = len(p_hier_graph.vs)-1\n",
    "            # print(l_sg_vrtx_indx)\n",
    "            l_node_cnt += 1\n",
    "            p_hier_graph.vs[\"layer\"] = [\"Layer \"+str(l_edge_threshold) if x is None else x for x in p_hier_graph.vs[\"layer\"]]\n",
    "            p_hier_graph.vs[\"layer_n\"] = [l_edge_threshold if x is None else x for x in p_hier_graph.vs[\"layer_n\"]]\n",
    "            p_hier_graph.vs[\"graph\"] = [sg if x is None else x for x in p_hier_graph.vs[\"graph\"]]\n",
    "            p_hier_graph.vs[\"parent_node\"] = [\"n\"+str(l_prnt_index)  if x is None else x for x in p_hier_graph.vs[\"parent_node\"]]\n",
    "\n",
    "            p_hier_graph.add_edge(p_parent_vtx, l_vrtx_name)\n",
    "\n",
    "            # Recursion\n",
    "            l_next_edge_threshold = l_edge_threshold+p_edge_th_step\n",
    "            #Условие входа в рекурсию:\n",
    "            #создавали узел\n",
    "            #максимальное число шагов не достигнуто\n",
    "            if (len(sg.vs)>1) and (p_layer_n < p_max_layer_cnt) and (l_next_edge_threshold<1) and (l_next_edge_threshold < p_max_edge_threshold):\n",
    "              # del_edges(sg, p_edge_threshold)\n",
    "              l_sg_keywords = add_layer(p_hier_graph, sg, l_vrtx_name, p_layer_n+1, l_next_edge_threshold, p_edge_th_step, p_max_edge_threshold,\n",
    "                                        p_max_layer_cnt, p_min_subgraph_coeff, p_keywords_cnt, p_keyword_coeff, p_avg_bouquet)\n",
    "              i = 0\n",
    "              for k,v in l_sg_keywords.items(): #пополним список ключевых слов родительской вершины\n",
    "                  if i == 0:\n",
    "                    l_keywords[k] = v*p_keyword_coeff\n",
    "                  else:\n",
    "                    l_keywords[k] = v\n",
    "                  i += 1\n",
    "            else: # в рекурсию не вошли, значит просто пополняем список ключевых слов родителя\n",
    "              i = 0\n",
    "              for w in get_subgraph_keywords(sg, p_keywords_cnt):\n",
    "                if i == 0:\n",
    "                    l_keywords[w] = 1*p_keyword_coeff\n",
    "                else:\n",
    "                    l_keywords[w] = 1\n",
    "                l_sg_keywords[w] = 1\n",
    "                i += 1\n",
    "\n",
    "            # Для добавленного узла нужно вставить его ключевые слова и количество детей\n",
    "            l_words = ' '.join(l_sg_keywords.keys())\n",
    "            # print(l_words)\n",
    "            # print(p_hier_graph.vs[\"keywords\"][0:5])\n",
    "            p_hier_graph.vs[\"keywords\"]  = [l_words if i == l_sg_vrtx_indx else p_hier_graph.vs[\"keywords\"][i]\n",
    "                                            for i in range(0, len(p_hier_graph.vs))\n",
    "                                           ]\n",
    "            # print(p_hier_graph.vs[\"keywords\"][0:5])\n",
    "            p_hier_graph.vs[\"child_node_cnt\"]  = [len(sg.vs()) if i == l_sg_vrtx_indx else p_hier_graph.vs[\"child_node_cnt\"][i]\n",
    "                                                  for i in range(0, len(p_hier_graph.vs))\n",
    "                                                 ]\n",
    "\n",
    "        # если вес данного подграфа НЕ достин собственного узла в иерархии то просто пополняем ключевые слова родителя\n",
    "        else:\n",
    "            # just add keywords\n",
    "            i = 0\n",
    "            for w in get_subgraph_keywords(sg, p_keywords_cnt):\n",
    "                if i == 0:\n",
    "                    l_keywords[w] = 1*p_keyword_coeff\n",
    "                else:\n",
    "                    l_keywords[w] = 1\n",
    "                i += 1\n",
    "\n",
    "    # if p_layer_n == 1:\n",
    "    #     print(l_keywords)\n",
    "    l_list = sorted(l_keywords, key=l_keywords.get, reverse=True)[0:p_keywords_cnt] # у нас уже столько сколько нужно слов\n",
    "\n",
    "\n",
    "    return {k: l_node_cnt for k in l_list}\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "\n",
    "def getEdgeStat(pGraph: igraph.Graph) -> [float, float, float]:\n",
    "\n",
    "    if len(pGraph.es[\"weight\"]) == 0:\n",
    "        return [1,1,1]\n",
    "\n",
    "    l_max = max(w for w in  pGraph.es[\"weight\"])\n",
    "    l_min = min(w for w in  pGraph.es[\"weight\"])\n",
    "    l_avg = sum(w for w in  pGraph.es[\"weight\"])/len(pGraph.es[\"weight\"])\n",
    "\n",
    "    return [l_max, l_min, l_avg]\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "\n",
    "def getVertexStat(pGraph: igraph.Graph) -> float:\n",
    "\n",
    "    l_sum = sum(vw for vw in p_graph.vs[\"norm_weight\"])\n",
    "\n",
    "    return [l_min, l_avg, l_max]\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "#---------------------------------------------------------------------------\n",
    "\n",
    "# main part\n",
    "print(\"--- Start time %s ---\" % strftime(\"%a, %d %b %Y %H:%M:%S\", gmtime()))\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "#parameters\n",
    "l_th = g_th  #0.4\n",
    "l_max_inds_count = g_max_inds_count #19995 # для определения имени файлов для чтения и запии\n",
    "l_th_start = l_th # старовый уровень уничтоженных ребер\n",
    "l_th_step = 0.01 # шаг рекурсии по уничтожению ребер\n",
    "l_th_max = 0.99 # максимальный уровень до которого уничтожаем ребра\n",
    "l_max_depth = 1000 #максимальная глубина рекурсии\n",
    "l_avg_bouquet = 3 # целевое количество подузлов в дереве\n",
    "\n",
    "l_min_subgraph_coeff = 9.0 #коэффициент при котором субграф добавляется в иерархию\n",
    "l_keywords_cnt = 10 #количество ключевых слов определяющих узел\n",
    "l_keyword_coeff = 100 # множитель для первого слова в ключевых словах от каждого узла (чтобы один узел не затмил своими словами другие)\n",
    "\n",
    "#load graph\n",
    "l_graph_main = igraph.Graph().Load(g_path+\"monocity_words_graph_\"\n",
    "                        +str(l_max_inds_count)+\"_\"+str(round(l_th*100))+\".graphml\"\n",
    "                       )\n",
    "print(\"Main graph summary: \", l_graph_main.summary())\n",
    "\n",
    "l_hier_graph = igraph.Graph() # результирующий иерархический граф\n",
    "l_hier_graph.add_vertices(1)\n",
    "l_hier_graph.vs[\"name\"] = [\"_Моногорода_\"]\n",
    "l_hier_graph.vs[\"keywords\"] = [\"_Моногорода_\"]\n",
    "l_hier_graph.vs[\"layer\"] = [\"Layer \"+str(0)]\n",
    "l_hier_graph.vs[\"layer_n\"] = [0]\n",
    "l_hier_graph.vs[\"child_node_cnt\"] = [len(l_graph_main.vs)]\n",
    "l_hier_graph.vs[\"parent_node\"] = [\"n\"]\n",
    "l_hier_graph.vs[\"graph\"] = [l_graph_main]\n",
    "# l_hier_graph.vs[\"child_cnt\"] = [-1]\n",
    "# l_hier_graph.vs[\"child\"] = [[]]\n",
    "\n",
    "\n",
    "l_layer = 1\n",
    "l_parent_vrtx_name = \"_Моногорода_\"\n",
    "\n",
    "# l_comps = l_graph_main.components(mode='STRONG')  #Returns:a VertexClustering object\n",
    "# l_layer += 1\n",
    "l_mono_dict = add_layer(l_hier_graph, l_graph_main, l_parent_vrtx_name, l_layer,\n",
    "                        l_th_start, l_th_step, l_th_max,\n",
    "                        l_max_depth,\n",
    "                        l_min_subgraph_coeff, l_keywords_cnt, l_keyword_coeff,\n",
    "                        l_avg_bouquet\n",
    "                       )\n",
    "# print(add_layer(l_hier_graph, l_graph_main, l_parent_vrtx_name, l_layer, 0.75, 0.95, 0.05, 7))\n",
    "l_words = ' '.join(l_mono_dict.keys())\n",
    "l_hier_graph.vs[\"keywords\"]  = [l_words if i == 0 else l_hier_graph.vs[\"keywords\"][i]\n",
    "                                for i in range(0, len(l_hier_graph.vs))\n",
    "                               ]\n",
    "l_hier_graph.vs[\"child_cnt\"]  = l_hier_graph.degree()\n",
    "\n",
    "# l_hier_graph.vs[\"child\"] = [l_words if i == 0 else l_hier_graph.vs[\"child\"][i]\n",
    "#                             for i in range(0, len(l_hier_graph.vs))\n",
    "#                            ]\n",
    "\n",
    "print(\"Hier graph summary: \", l_hier_graph.summary())\n",
    "print(\"Graphs' samples: \", l_hier_graph.vs[\"parent_node\"][0:15])\n",
    "print(\"Graphs' in hierarchy: \", len(l_hier_graph.vs[\"graph\"]))\n",
    "print(\"Graphs' samples: \", l_hier_graph.vs[\"graph\"][0:5])\n",
    "print(\"One graph words sample: \", l_hier_graph.vs[\"graph\"][0].vs[\"name\"][0:10])\n",
    "\n",
    "\n",
    "#Вычислим дополнительные полезные характеристики\n",
    "# - суммарный вес слов\n",
    "# - мин макс и среднее значение веса ребер\n",
    "\n",
    "\n",
    "l_hier_graph.vs[\"subgraph_weigth\"] = [get_subgraph_weight(sg)  for sg in  l_hier_graph.vs[\"graph\"]]\n",
    "l_sg_edge_stats = [getEdgeStat(sg)  for sg in l_hier_graph.vs[\"graph\"]]\n",
    "\n",
    "\n",
    "l_hier_graph.vs[\"subgraph_edge_mins\"] = [i[0] for i in l_sg_edge_stats]\n",
    "l_hier_graph.vs[\"subgraph_edge_avgs\"] = [i[1] for i in l_sg_edge_stats]\n",
    "l_hier_graph.vs[\"subgraph_edge_maxs\"] = [i[2] for i in l_sg_edge_stats]\n",
    "\n",
    "\n",
    "l_hier_graph.write_graphml(g_path+\"monocity_words_hier_graph_\"\n",
    "                           +str(l_max_inds_count)+\"_\"+str(round(l_th*100))+\".graphml\"\n",
    "                          )\n",
    "\n",
    "print(\"Graph file: \"+g_path+\"monocity_words_hier_graph_\"\n",
    "      +str(l_max_inds_count)+\"_\"+str(round(l_th*100))+\".graphml\"\n",
    "     )\n",
    "print(\"Done\")\n",
    "end_time = time.time()\n",
    "print(\"--- %s seconds ---\" % (end_time - start_time))\n",
    "#---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Start time Thu, 06 Apr 2017 14:45:25 ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inds len:  19995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inds sample [0:10]:  [[0, 'ДОЛЯ НАСЕЛЕНИЯ ИМЕЮЩЕГО ДЕНЕЖНЫЕ ДОХОДЫ НИЖЕ ВЕЛИЧИНЫ ПРОЖИТОЧНОГО МИНИМУМА В ОБЩЕЙ ЧИСЛЕННОСТИ НАСЕЛЕНИЯ АЛЕЙСКОГО РАЙОНА АЛТАЙСКОГО КРАЯ', ['население', 'денежный', 'доход', 'ниже', 'величина', 'прожиточный', 'население', 'алейский', 'район', 'край'], {0: 0.8, 1: 0.8, 2: 0.8, 3: 0.7, 4: 0.6, 5: 0.6, 6: 0.5, 7: 0.4, 8: 0.2}], [1, 'КОЛИЧЕСТВО ИСТОРИКОПАТРИОТИЧЕСКИХ ГЕРОИКОПАТРИОТИЧЕСКИХ И ВОЕННОПАТРИОТИЧЕСКИХ МУЗЕЕВ ИЛИ МУЗЕЙНЫХ КОМНАТ В ОБЩЕОБРАЗОВАТЕЛЬНЫХ УЧРЕЖДЕНИЯХ', ['историкопатриотический', 'героикопатриотический', 'военнопатриотический', 'музей', 'музейный', 'комната', 'общеобразовательный'], {0: 0.571, 1: 0.571, 2: 0.571, 3: 0.571, 4: 0.571, 5: 0.571, 6: 0.571, 7: 0.571, 8: 0.571, 9: 0.286, 10: 0.286, 103: 0.286, 12: 0.143, 66: 0.143, 81: 0.143, 104: 0.286}], [2, 'ОХВАТ ПРОФИЛАКТИЧЕСКИМИ МЕРАМИ ПОДРОСТКОВ И МОЛОДЕЖИ В ВОЗРАСТЕ ОТ ДО ЛЕТ', ['охват', 'профилактический', 'подросток', 'молодёжь', 'возраст'], {0: 1.0, 1: 1.0, 2: 0.8, 3: 0.6, 4: 0.6, 5: 0.6, 6: 0.4, 7: 0.4, 8: 0.4, 9: 0.4, 142: 0.2, 89: 0.2}], [3, 'УРОВЕНЬ ЗАНЯТОСТИ НАСЕЛЕНИЯ', ['занятость', 'население'], {0: 1.0, 1: 1.0, 2: 1.0, 3: 0.5, 4: 0.5, 5: 0.5, 6: 0.5, 7: 0.5, 116: 0.5, 114: 0.5}], [4, 'УДЕЛЬНЫЙ ВЕС РАБОТНИКОВ С ПРОФЕССИОНАЛЬНЫМ ОБРАЗОВАНИЕМ В ОБЩЕЙ ЧИСЛЕННОСТИ ЗАНЯТЫХ В ЭКОНОМИКЕ', ['вес', 'работник', 'профессиональный', 'занятой', 'экономика'], {0: 0.8, 1: 0.8, 2: 0.8, 3: 0.8, 4: 0.6, 5: 0.4, 6: 0.4, 7: 0.4, 8: 0.4, 9: 0.4, 10: 0.4, 75: 0.2, 12: 0.2, 76: 0.2, 69: 0.2}], [5, 'ДОЛЯ ВЫПУСКНИКОВ ОБЩЕОБРАЗОВАТЕЛЬНЫХ ОРГАНИЗАЦИЙ ПОСТУПИВШИХ В ПРОФЕССИОНАЛЬНЫЕ ОБРАЗОВАТЕЛЬНЫЕ ОРГАНИЗАЦИИ', ['выпускник', 'общеобразовательный', 'поступить', 'профессиональный', 'образовательный'], {0: 1.0, 1: 1.0, 2: 1.0, 3: 0.8, 4: 0.8, 5: 0.6, 6: 0.6, 7: 0.6, 8: 0.6, 9: 0.6, 10: 0.6, 12: 0.4, 64: 0.2, 81: 0.2, 61: 0.4, 62: 0.2, 63: 0.2}], [6, 'ОБЪЕМ ОТГРУЖЕННЫХ ТОВАРОВ СОБСТВЕННОГО ПРОИЗВОДСТВА ВЫПОЛНЕННЫХ РАБОТ УСЛУГ СОБСТВЕННЫМИ СИЛАМИ ПО КРУГУ КРУПНЫХ И СРЕДНИХ ОРГАНИЗАЦИЙ', ['объесть', 'отгрузить', 'товар', 'собственный', 'производство', 'выполнить', 'работа', 'услуга', 'собственный', 'сила', 'круг', 'крупный'], {0: 0.667, 1: 0.667, 2: 0.583, 3: 0.417, 4: 0.333, 5: 0.333, 6: 0.25, 7: 0.167, 8: 0.167, 9: 0.167, 10: 0.167}], [7, 'ОБЪЕМ ИНВЕСТИЦИЙ ЗА СЧЕТ ВСЕХ ИСТОЧНИКОВ ФИНАНСИРОВАНИЯ БЕЗ СУБЪЕКТОВ МАЛОГО ПРЕДПРИНИМАТЕЛЬСТВА И ОБЪЕМОВ ИНВЕСТИЦИЙ НЕ НАБЛЮДАЕМЫХ ПРЯМЫМИ СТАТИСТИЧЕСКИМИ МЕТОДАМИ', ['объесть', 'инвестиция', 'счёт', 'источник', 'финансирование', 'субъект', 'предпринимательство', 'инвестиция', 'наблюдать', 'прямая', 'статистический', 'метод'], {0: 0.667, 1: 0.667, 2: 0.667, 3: 0.583, 4: 0.5, 5: 0.5, 6: 0.417, 7: 0.25, 8: 0.25, 9: 0.167, 10: 0.167, 12: 0.167}], [8, 'УВЕЛИЧЕНИЕ ЧИСЛА ДЕТЕЙ ОЗДОРОВЛЕННЫХ В ПОЕЗДКАХ С РОДИТЕЛЯМИ', ['ребёнок', 'оздоровить', 'поездка', 'родитель'], {0: 0.75, 1: 0.75, 2: 0.5, 3: 0.5, 4: 0.5, 5: 0.5, 6: 0.5, 7: 0.5, 8: 0.5, 9: 0.5, 148: 0.25, 87: 0.25, 91: 0.25}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indicators clustering results saved in  D:/Projects/Aspirantura/GASU_SP/monocity_inds_hier_19995.csv\nDone\n--- 344.57845425605774 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Indicators fuzzy clustering\n",
    "# выполняется всегда сразу после предыдущего пункта, т.к. тут не загружается граф иерархический\n",
    "\n",
    "import igraph\n",
    "# from igraph import *\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "from time import gmtime, strftime\n",
    "from typing import List\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "# main part\n",
    "print(\"--- Start time %s ---\" % strftime(\"%a, %d %b %Y %H:%M:%S\", gmtime()))\n",
    "start_time = time.time()\n",
    "\n",
    "#parameters\n",
    "l_th = g_th #0.4\n",
    "l_max_inds_count = g_max_inds_count #19995 # для определения имени файлов для чтения и запии\n",
    "# l_th_start = l_th # старовый уровень уничтоженных ребер\n",
    "# l_th_step = 0.01 # шаг рекурсии по уничтожению ребер\n",
    "# l_th_max = 0.9 # максимальный уровень до которого уничтожаем ребра\n",
    "# l_max_depth = 20 #максимальная глубина рекурсии\n",
    "#\n",
    "# l_min_subgraph_coeff = 3.0 #коэффициент при котором субграф добавляется в иерархию\n",
    "# l_keywords_cnt = 10 #количество ключевых слов определяющих узел\n",
    "# l_keyword_coeff = 100 # множитель для первого слова в ключевых словах от каждого узла (чтобы один узел не затмил своими словами другие)\n",
    "\n",
    "#load graph\n",
    "# l_hier_graph = igraph.Graph().Load(\"D:/Projects/Aspirantura/GASU_SP/monocity_words_hier_graph_\"\n",
    "#                         +str(l_max_inds_count)+\"_\"+str(round(l_th*100))+\".graphml\"\n",
    "#                        )\n",
    "\n",
    "\n",
    "#load indicators\n",
    "pkl_file = open(g_path+'monocity_inds_'+str(l_max_inds_count)+'.pkl', 'rb')\n",
    "inds = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "print(\"Inds len: \", len(inds))\n",
    "\n",
    "for v in l_hier_graph.vs:\n",
    "    for i in inds:\n",
    "        if len(i[2])>0:\n",
    "            l_cnt = 0\n",
    "            l_index = v.index\n",
    "            l_graphs = l_hier_graph.vs[\"graph\"]\n",
    "            l_graph = l_graphs[l_index]\n",
    "            for w in l_graph.vs:\n",
    "                if w[\"name\"] in i[2]:\n",
    "                    l_cnt += 1\n",
    "            # print(i[0])\n",
    "            l_pcnt = l_cnt/len(i[2])\n",
    "            if l_pcnt > 0.1:\n",
    "                i[3][l_index] = round(l_pcnt,3)\n",
    "\n",
    "\n",
    "print(\"Inds sample [0:10]: \", inds[0:9])\n",
    "\n",
    "#write indicators\n",
    "output = open(g_path+'monocity_inds_hier_'+str(l_max_inds_count)+'.pkl', 'wb')\n",
    "pickle.dump(inds, output)\n",
    "output.close()\n",
    "\n",
    "#write indicators to csv\n",
    "l_df = pd.DataFrame(inds)\n",
    "l_df.to_csv(g_path+'monocity_inds_hier_'+str(g_max_inds_count)+'.csv', index=False, header=True)\n",
    "print(\"Indicators clustering results saved in \", g_path+'monocity_inds_hier_'+str(g_max_inds_count)+'.csv')\n",
    "\n",
    "\n",
    "print(\"Done\")\n",
    "end_time = time.time()\n",
    "print(\"--- %s seconds ---\" % (end_time - start_time))\n",
    "#---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Start time Fri, 31 Mar 2017 18:13:44 ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID:  596  keywords:  футбольный хоккей хоккейный волейбол баскетбол футбол\nInds cnt:  12\nInds sample [0:10]:\n0.25  -  258  -  ДОЛЯ НАСЕЛЕНИЯ СИСТЕМАТИЧЕСКИ ЗАНИМАЮЩЕГОСЯ ФУТБОЛОМ \n0.25  -  3644  -  ПРОВЕДЕНИЕ ПЕРВЕНСТВА ДЮСШ ПО БАСКЕТБОЛУ\n0.25  -  10626  -  КОЛИЧЕСТВО ФУТБОЛЬНЫХ СТАДИОНОВ СООТВЕТСТВУЮЩИХ ГОСУДАРСТВЕННЫМ ТРЕБОВАНИЯМ \n0.25  -  11067  -  ПРОВЕДЕНИЕ ПЕРВЕНСТВА ДЮСШ ПО ФУТБОЛУ\n0.25  -  16038  -  ОТКРЫТОЕ ПЕРВЕНСТВО ГОРОДА ПО ВОЛЕЙБОЛУ\n0.25  -  18596  -  ПРОВЕДЕНИЕ ПЕРВЕНСТВА ДЮСШ ПО ВОЛЕЙБОЛУ\n0.2  -  1256  -  РЕЗУЛЬТАТ ВЫСТУПЛЕНИЙ В СЕЗОНЕ ХОККЕЙНОГО КЛУБА\n0.2  -  3753  -  РЕЗУЛЬТАТ ВЫСТУПЛЕНИЙ В СЕЗОНЕ ФУТБОЛЬНОГО КЛУБА\n0.2  -  16035  -  СОРЕВНОВАНИЯ ПО ВОЛЕЙБОЛУ ПОСВЯЩЕННЫЕ ДНЮ ЗАЩИТНИКА\n0.182  -  11152  -  КОЛИЧЕСТВО ТРЕНЕРОВПРЕПОДАВАТЕЛЕЙ ОКАЗЫВАЮЩИХ УСЛУГИ ПО ПРОВЕДЕНИЮ УЧЕБНОТРЕНИРОВОЧНЫХ ЗАНЯТИЙ ПО ХОККЕЮ С ШАЙБОЙ ФИГУРНОМУ КАТАНИЮ И ФУТБОЛУ\n----------------------------------\nNode ID:  600  keywords:  клуб команда сезон\nInds cnt:  65\nInds sample [0:10]:\n0.4  -  1256  -  РЕЗУЛЬТАТ ВЫСТУПЛЕНИЙ В СЕЗОНЕ ХОККЕЙНОГО КЛУБА\n0.4  -  3753  -  РЕЗУЛЬТАТ ВЫСТУПЛЕНИЙ В СЕЗОНЕ ФУТБОЛЬНОГО КЛУБА\n0.333  -  3477  -  КОЛИЧЕСТВО СБОРНЫХ КОМАНД ГОРОДА\n0.333  -  6566  -  КОЛИЧЕСТВО ДЕТСКИХ И ПОДРОСТКОВЫХ КЛУБОВ\n0.333  -  7672  -  ОРГАНИЗАЦИЯ РАБОТЫ И ПОДДЕРЖКА КЛУБОВ \n0.25  -  1228  -  ЧИСЛЕННОСТЬ СПОРТСМЕНОВ ВХОДЯЩИХ В СБОРНЫЕ КОМАНДЫ РАЗЛИЧНОГО УРОВНЯ\n0.25  -  4954  -  КОЛИЧЕСТВО ДЕЙСТВУЮЩИХ ПАТРИОТИЧЕСКИХ ОБЪЕДИНЕНИЙ КЛУБОВ \n0.25  -  7620  -  ДОЛЯ НАСЕЛЕНИЯ МУНИЦИПАЛЬНОГО ОБРАЗОВАНИЯ ПОСЕЩАЮЩИХ МОЛОДЕЖНЫЕ КЛУБЫ\n0.25  -  10349  -  КОЛИЧЕСТВО СОЗДАННЫХ ДОБРОВОЛЬНЫХ ПОЖАРНЫХ КОМАНД\n0.25  -  10695  -  УВЕЛИЧЕНИЕ УЧАСТНИКОВ КРУЖКОВ КЛУБОВ ПО ИНТЕРЕСАМ \n----------------------------------\nNode ID:  601  keywords:  тренировочный тренировка упражнение\nInds cnt:  6\nInds sample [0:10]:\n0.25  -  10627  -  КОЛИЧЕСТВО ТРЕНИРОВОЧНЫХ ПЛОЩАДОК СООТВЕТСТВУЮЩИХ ГОСУДАРСТВЕННЫМ ТРЕБОВАНИЯМ\n0.2  -  13254  -  КОЛИЧЕСТВО ПРОВЕДЕННЫХ ТРЕНИРОВОК СОРЕВНОВАНИЙ СМОТРОВКОНКУРСОВ В ОБЛАСТИ ГО И ЧС\n0.167  -  1671  -  ПРОВЕДЕНИЕ ТРЕНИРОВОЧНЫХ УЧЕНИЙ С СИЛАМИ РСЧС РАЙОНА\n0.143  -  12772  -  ДОЛЯ ГОРОЖАН ПОДДЕРЖИВАЮЩИХ СОБСТВЕННОЕ ЗДОРОВЬЕ ПРИ ПОМОЩИ ФИЗИЧЕСКИХ УПРАЖНЕНИЙ\n0.125  -  19024  -  КОЛИЧЕСТВО ПРОВЕДЕННЫХ ТРЕНИРОВОК СОРЕВНОВАНИЙ СМОТРОВКОНКУРСОВ В ОБЛАСТИ ПРЕДУПРЕЖДЕНИЯ И ЛИКВИДАЦИИ ЧРЕЗВЫЧАЙНЫХ СИТУАЦИЙ \n0.111  -  15807  -  КОЛИЧЕСТВО ПРОВЕДЕННЫХ РАЙОННЫХ ТРЕНИРОВОК СОРЕВНОВАНИЙ СМОТРОВКОНКУРСОВ В ОБЛАСТИ ПРЕДУПРЕЖДЕНИЯ И ЛИКВИДАЦИИ ЧРЕЗВЫЧАЙНЫХ СИТУАЦИЙ\n----------------------------------\nNode ID:  594  keywords:  олимпиада тренировочный футбол футзал соревнование кубок физкультурник хоккей физкультурный первенство\nInds cnt:  478\nInds sample [0:10]:\n1.0  -  17981  -  КОЛИЧЕСТВО СПОРТИВНЫХ УЧРЕЖДЕНИЙ\n0.667  -  6159  -  ЧЕМПИОНАТ И ПЕРВЕНСТВО РК ПО КАРАТЭ\n0.667  -  14359  -  КОЛИЧЕСТВО ФИЗКУЛЬТУРНЫХ И СПОРТИВНЫХ МЕРОПРИЯТИЙ\n0.667  -  15306  -  КОЛИЧЕСТВО КОМАНД УЧАСТВУЮЩИХ В КУБКЕ И ЧЕМПИОНАТЕ РОССИИ ПО ВОЛЕЙБОЛУ\n0.667  -  16868  -  КОЛИЧЕСТВО ФИЗКУЛЬТУРНЫХ И СПОРТИВНЫХ МЕРОПРИЯТИЙ\n0.667  -  18600  -  ЧЕМПИОНАТ И ПЕРВЕНСТВО РК ПО КАРАТЭ \n0.6  -  1256  -  РЕЗУЛЬТАТ ВЫСТУПЛЕНИЙ В СЕЗОНЕ ХОККЕЙНОГО КЛУБА\n0.6  -  3753  -  РЕЗУЛЬТАТ ВЫСТУПЛЕНИЙ В СЕЗОНЕ ФУТБОЛЬНОГО КЛУБА\n0.5  -  1075  -  КОЛИЧЕСТВО КЛУБНЫХ ФОРМИРОВАНИЙ\n0.5  -  1600  -  УВЕЛИЧЕНИЕ ЧИСЛА МЕРОПРИЯТИЙ ПРОВОДИМЫХ КЛУБНЫМИ УЧРЕЖДЕНИЯМИ\n----------------------------------\nDone\n--- 1.1240642070770264 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# test fuzzy hierarchical clustering results\n",
    "\n",
    "import igraph\n",
    "# from igraph import *\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "from time import gmtime, strftime\n",
    "from typing import List\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "#parameters\n",
    "l_th = g_th\n",
    "l_max_inds_count = g_max_inds_count # для определения имени файлов для чтения и запии\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "def getIndicatorsByNode(pNodeIndex: int, pInds: list) -> []:\n",
    "    l_list=[]\n",
    "    for i in pInds:\n",
    "        l_pt = i[3].get(pNodeIndex, None)\n",
    "        if l_pt != None:\n",
    "            l_list.append([i, l_pt])\n",
    "\n",
    "    l_list.sort(key=lambda arg: arg[1],  reverse=True)\n",
    "\n",
    "    return l_list\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "# main part\n",
    "print(\"--- Start time %s ---\" % strftime(\"%a, %d %b %Y %H:%M:%S\", gmtime()))\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# load graph\n",
    "l_hier_graph = igraph.Graph().Load(g_path+'monocity_words_hier_graph_'+str(g_max_inds_count)+\"_\"+str(round(g_th*100))+\".graphml\")\n",
    "\n",
    "#load indicators\n",
    "pkl_file = open(g_path + 'monocity_inds_hier_'+str(l_max_inds_count)+'.pkl', 'rb')\n",
    "inds = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "# print(\"Inds len: \", len(inds))\n",
    "\n",
    "\n",
    "# for n in [65, 83, 85, 91, 93, 41]:\n",
    "for n in [596, 600, 601, 594]:\n",
    "    print(\"Node ID: \", n, \" keywords: \", l_hier_graph.vs[\"keywords\"][n])\n",
    "    l_inds = getIndicatorsByNode(n, inds)\n",
    "    print(\"Inds cnt: \", len(l_inds))\n",
    "    print(\"Inds sample [0:10]:\")\n",
    "    for i in l_inds[0:10]:\n",
    "        print(i[1],\" - \",i[0][0],\" - \",i[0][1])\n",
    "    print(\"----------------------------------\")\n",
    "\n",
    "\n",
    "print(\"Done\")\n",
    "end_time = time.time()\n",
    "print(\"--- %s seconds ---\" % (end_time - start_time))\n",
    "#---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Start time Thu, 06 Apr 2017 14:51:19 ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph file: D:/Projects/Aspirantura/GASU_SP/monocity_words_hier_graph_19995_40.graphml\nDone\n--- 2.374237537384033 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Для более содержательного анализа было бы хорошо модифицировать наш граф, добавив туда еще два свойства\n",
    "# - количесво показателей ассоциированных с вершний\n",
    "# - общий вес показателей ассоциированных с вершиной\n",
    "\n",
    "import igraph\n",
    "# from igraph import *\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "from time import gmtime, strftime\n",
    "from typing import List\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "\n",
    "#parameters\n",
    "l_th = g_th\n",
    "l_max_inds_count = g_max_inds_count # для определения имени файлов для чтения и запии\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "\n",
    "def getIndicatorsByNode(pNodeIndex: int, pInds: list) -> []:\n",
    "    l_list=[]\n",
    "    for i in pInds:\n",
    "        l_pt = i[3].get(pNodeIndex, None)\n",
    "        if l_pt != None:\n",
    "            l_list.append([i, l_pt])\n",
    "\n",
    "    l_list.sort(key=lambda arg: arg[1],  reverse=True)\n",
    "\n",
    "    return l_list\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "# main part\n",
    "print(\"--- Start time %s ---\" % strftime(\"%a, %d %b %Y %H:%M:%S\", gmtime()))\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# load graph\n",
    "l_hier_graph = igraph.Graph().Load(g_path+'monocity_words_hier_graph_'+str(g_max_inds_count)+\"_\"+str(round(g_th*100))+\".graphml\")\n",
    "\n",
    "#load indicators\n",
    "pkl_file = open(g_path + 'monocity_inds_hier_'+str(l_max_inds_count)+'.pkl', 'rb')\n",
    "inds = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "# print(\"Inds len: \", len(inds))\n",
    "\n",
    "l_inds_cnt = []\n",
    "l_ind_weigths = []\n",
    "\n",
    "for v in l_hier_graph.vs():\n",
    "    l_inds = getIndicatorsByNode(v.index, inds)\n",
    "    l_inds_cnt.append(len(l_inds))\n",
    "    l_ind_weigths.append(sum(i[1] for i in l_inds))\n",
    "\n",
    "l_hier_graph.vs[\"inds_cnt\"] = l_inds_cnt\n",
    "l_hier_graph.vs[\"inds_weigth\"] = l_ind_weigths\n",
    "\n",
    "l_hier_graph.write_graphml(g_path+\"monocity_words_hier_graph_\"\n",
    "                           +str(l_max_inds_count)+\"_\"+str(round(l_th*100))+\".graphml\"\n",
    "                          )\n",
    "\n",
    "print(\"Graph file: \"+g_path+\"monocity_words_hier_graph_\"\n",
    "      +str(l_max_inds_count)+\"_\"+str(round(l_th*100))+\".graphml\"\n",
    "     )\n",
    "\n",
    "\n",
    "print(\"Done\")\n",
    "end_time = time.time()\n",
    "print(\"--- %s seconds ---\" % (end_time - start_time))\n",
    "#---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}